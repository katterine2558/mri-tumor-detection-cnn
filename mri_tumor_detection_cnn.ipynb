{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOBPVMx+GOU2Kbb9yIouT2C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/katterine2558/mri-tumor-detection-cnn/blob/main/mri_tumor_detection_cnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#==========================================================\n",
        "# LIBRARIES\n",
        "#==========================================================\n",
        "from google.colab import files\n",
        "#!pip install -q kaggle #Descomentar si no est√° instalado Kaggle en el entorno\n",
        "import os\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, Flatten, Resizing, Rescaling, RandomBrightness, RandomContrast, RandomRotation, RandomZoom, BatchNormalization, Dropout\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "from tensorflow.keras.metrics import AUC\n",
        "#!pip install keras-tuner --upgrade\n",
        "from keras_tuner.tuners import RandomSearch\n",
        "from tensorflow.keras import mixed_precision\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "\n",
        "\n",
        "#Ruta base\n",
        "base_dir = '/content/mri_data'\n",
        "#Ruta para almacenar split\n",
        "split_dir = \"/content/mri_data_split\"\n",
        "#Semilla\n",
        "seed = 0;\n",
        "#Tama√±o del batch\n",
        "batch_size = 32\n",
        "#Image size\n",
        "image_size = (224, 224)\n",
        "\n",
        "print(\"Dispositivo:\", tf.config.list_physical_devices('GPU'))\n",
        "mixed_precision.set_global_policy('mixed_float16')"
      ],
      "metadata": {
        "id": "GBZzywFcr1rD",
        "outputId": "85f79679-4694-4a48-df94-cb89817ea08c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dispositivo: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#==========================================================\n",
        "# DOWNLOAD DATASET\n",
        "#==========================================================\n",
        "def download_mri_data(base_dir:str):\n",
        "\n",
        "  files.upload() #APIKey de Kaggle\n",
        "\n",
        "  !mkdir -p ~/.kaggle\n",
        "  !mv kaggle.json ~/.kaggle/\n",
        "  !chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "  # Descargar el dataset\n",
        "  !mkdir -p /content/mri_data\n",
        "  !kaggle datasets download -d rm1000/brain-tumor-mri-scans -p /content/mri_data --unzip\n",
        "\n",
        "  # Listar categor√≠as\n",
        "  categories = os.listdir(base_dir)\n",
        "  print(\"Categor√≠as encontradas:\", categories)\n",
        "\n",
        "  # Contar im√°genes por clase\n",
        "  for c in categories:\n",
        "      path = os.path.join(base_dir, c)\n",
        "      print(f\"{c}: {len(os.listdir(path))} im√°genes\")\n",
        "\n",
        "  fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
        "  for i, c in enumerate(categories):\n",
        "      folder = os.path.join(base_dir, c)\n",
        "      img_name = random.choice(os.listdir(folder))\n",
        "      img_path = os.path.join(folder, img_name)\n",
        "      img = cv2.imread(img_path)\n",
        "      img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "      axes[i].imshow(img)\n",
        "      axes[i].set_title(c.capitalize())\n",
        "      axes[i].axis('off')\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "#Descarga el conjunto de datos\n",
        "download_mri_data(base_dir)"
      ],
      "metadata": {
        "id": "JQOc_LcWsUpH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 342
        },
        "outputId": "1fe8e3d3-88b3-495b-b866-d3fa22bf2002"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f25cdad6-3841-4ab4-989a-79d3bb27d9c0\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-f25cdad6-3841-4ab4-989a-79d3bb27d9c0\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-576069023.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;31m#Descarga el conjunto de datos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0mdownload_mri_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-576069023.py\u001b[0m in \u001b[0;36mdownload_mri_data\u001b[0;34m(base_dir)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdownload_mri_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_dir\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#APIKey de Kaggle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mkdir -p ~/.kaggle'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m(target_dir)\u001b[0m\n\u001b[1;32m     70\u001b[0m   \"\"\"\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m   \u001b[0muploaded_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_upload_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmultiple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m   \u001b[0;31m# Mapping from original filename to filename as saved locally.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m   \u001b[0mlocal_filenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36m_upload_files\u001b[0;34m(multiple)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m   \u001b[0;31m# First result is always an indication that the file picker has completed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m   result = _output.eval_js(\n\u001b[0m\u001b[1;32m    165\u001b[0m       'google.colab._files._uploadFiles(\"{input_id}\", \"{output_id}\")'.format(\n\u001b[1;32m    166\u001b[0m           \u001b[0minput_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result, timeout_sec)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     if (\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#==========================================================\n",
        "# IMAGE SIZE SCANNER\n",
        "#==========================================================\n",
        "\n",
        "def get_images_size(base_dir:str):\n",
        "\n",
        "  all_sizes = []  # guardamos (ancho, alto)\n",
        "  per_class_count = {}\n",
        "\n",
        "  for cls in os.listdir(base_dir):\n",
        "      cls_path = os.path.join(base_dir, cls)\n",
        "      if not os.path.isdir(cls_path):\n",
        "          continue\n",
        "\n",
        "      imgs = os.listdir(cls_path)\n",
        "      per_class_count[cls] = len(imgs)\n",
        "\n",
        "      # vamos a mirar hasta 20 im√°genes por clase (para no leer miles)\n",
        "      for img_name in imgs[:20]:\n",
        "          img_path = os.path.join(cls_path, img_name)\n",
        "          img = cv2.imread(img_path)  # esto lee en BGR\n",
        "          if img is None:\n",
        "              # a veces hay archivos raros tipo .txt o corruptos\n",
        "              continue\n",
        "          h, w, ch = img.shape  # alto, ancho, canales\n",
        "          all_sizes.append((w, h, ch))\n",
        "\n",
        "\n",
        "  print(\"\\nEjemplos de tama√±os (ancho x alto x canales) en las primeras im√°genes revisadas:\")\n",
        "  for s in all_sizes[:10]:\n",
        "      print(\" \", s)\n",
        "\n",
        "  # estad√≠sticas m√°s globales\n",
        "  widths = [s[0] for s in all_sizes]\n",
        "  heights = [s[1] for s in all_sizes]\n",
        "  channels = [s[2] for s in all_sizes]\n",
        "\n",
        "  print(\"\\nEstad√≠sticas:\")\n",
        "  print(f\"  Ancho min / max / promedio: {min(widths)} / {max(widths)} / {np.mean(widths):.1f}\")\n",
        "  print(f\"  Alto  min / max / promedio: {min(heights)} / {max(heights)} / {np.mean(heights):.1f}\")\n",
        "\n",
        "  print(\"\\nCanales m√°s comunes (1=gris, 3=RGB):\")\n",
        "  print(Counter(channels))\n",
        "\n",
        "#Obtiene el tama√±o de las im√°genes\n",
        "get_images_size(base_dir)"
      ],
      "metadata": {
        "id": "uDvO1Exeyn1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#==========================================================\n",
        "# TRAIN-VAL-TEST DIVISION (70-20-10)\n",
        "#==========================================================\n",
        "\n",
        "def traint_val_test_split(base_dir:str, split_dir:str):\n",
        "\n",
        "  # Crear carpetas destino\n",
        "  for split in ['train', 'val', 'test']:\n",
        "      for cls in os.listdir(base_dir):\n",
        "          os.makedirs(os.path.join(split_dir, split, cls), exist_ok=True)\n",
        "\n",
        "  # Ratios\n",
        "  train_ratio = 0.7\n",
        "  val_ratio = 0.2\n",
        "  test_ratio = 0.1\n",
        "\n",
        "  # Procesar cada clase\n",
        "  for cls in os.listdir(base_dir):\n",
        "      cls_path = os.path.join(base_dir, cls)\n",
        "      images = [f for f in os.listdir(cls_path) if f.lower().endswith(('.png','.jpg','.jpeg'))]\n",
        "\n",
        "      # Dividir train y (val+test)\n",
        "      train_files, temp_files = train_test_split(images, test_size=(1-train_ratio), random_state=seed)\n",
        "\n",
        "      # Dividir en val y test\n",
        "      val_size = val_ratio / (val_ratio + test_ratio)\n",
        "      val_files, test_files = train_test_split(temp_files, test_size=(1-val_size), random_state=seed)\n",
        "\n",
        "      # Copiar archivos\n",
        "      for fname in train_files:\n",
        "          shutil.copy(os.path.join(cls_path, fname), os.path.join(split_dir, 'train', cls, fname))\n",
        "      for fname in val_files:\n",
        "          shutil.copy(os.path.join(cls_path, fname), os.path.join(split_dir, 'val', cls, fname))\n",
        "      for fname in test_files:\n",
        "          shutil.copy(os.path.join(cls_path, fname), os.path.join(split_dir, 'test', cls, fname))\n",
        "\n",
        "  print(\"Divisi√≥n completada con semilla =\", seed)\n",
        "  print(\"Estructura creada en:\", split_dir)\n",
        "\n",
        "  # Verificaci√≥n de conteo\n",
        "  for split in ['train', 'val', 'test']:\n",
        "      print(f\"\\n * {split.upper()}\")\n",
        "      for cls in os.listdir(os.path.join(split_dir, split)):\n",
        "          count = len(os.listdir(os.path.join(split_dir, split, cls)))\n",
        "          print(f\"  {cls}: {count} im√°genes\")\n",
        "\n",
        "#Divisi√≥n traint/val/test\n",
        "traint_val_test_split(base_dir, split_dir)\n",
        "\n",
        "#Carga los datos de validaci√≥n y test\n",
        "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "        f'{split_dir}/val',\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        seed=seed,\n",
        "        image_size=image_size\n",
        "    )\n",
        "test_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "        f'{split_dir}/test',\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        seed=seed,\n",
        "        image_size=image_size\n",
        "    )"
      ],
      "metadata": {
        "id": "cEMADKmdWuvX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# TRAIN DATA AUGMENTATION\n",
        "# ==========================================================\n",
        "\n",
        "def data_augmentation(split_dir:str, seed:int, batch_size:int, image_size:tuple):\n",
        "    # Cargar im√°genes (se necesita tama√±o fijo)\n",
        "    train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "        f'{split_dir}/train',\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        seed=seed,\n",
        "        image_size=image_size\n",
        "    )\n",
        "\n",
        "    class_names = train_ds.class_names\n",
        "    print(\"Clases detectadas:\", class_names)\n",
        "\n",
        "    # üîπ Crear mapeo expl√≠cito de √≠ndice ‚Üí nombre\n",
        "    label_map = {i: name for i, name in enumerate(class_names)}\n",
        "    print(\"\\nMapeo de etiquetas:\")\n",
        "    for key, value in label_map.items():\n",
        "        print(f\"{key} ‚Üí {value}\")\n",
        "\n",
        "    # Definir Data Augmentation\n",
        "    data_augmentation = tf.keras.Sequential([\n",
        "        RandomRotation(0.05),\n",
        "        RandomZoom(0.1),\n",
        "        RandomContrast(0.1),\n",
        "        RandomBrightness(0.1),\n",
        "    ])\n",
        "\n",
        "    # Aplicar aumentaci√≥n\n",
        "    train_ds = train_ds.map(lambda x, y: (data_augmentation(x, training=True), y))\n",
        "\n",
        "    print(\"\\n Data augmentation aplicada al conjunto de entrenamiento.\")\n",
        "    return train_ds, data_augmentation, class_names, label_map\n",
        "\n",
        "\n",
        "def plot_data_augmentation(train_ds, data_augmentation):\n",
        "    for images, labels in train_ds.take(1):\n",
        "        plt.figure(figsize=(10, 10))\n",
        "        for i in range(9):\n",
        "            augmented = data_augmentation(images, training=True)\n",
        "            ax = plt.subplot(3, 3, i + 1)\n",
        "            plt.imshow(augmented[i].numpy().astype(\"uint8\"))\n",
        "            plt.axis(\"off\")\n",
        "        plt.suptitle(\"Ejemplos de Data Augmentation\", fontsize=14)\n",
        "        plt.show()\n",
        "        break\n",
        "\n",
        "\n",
        "# Ejecutar data augmentation\n",
        "train_ds, data_aug, class_names, label_map = data_augmentation(split_dir, seed, batch_size, image_size)\n",
        "plot_data_augmentation(train_ds, data_aug)"
      ],
      "metadata": {
        "id": "hLR5Mm83cO1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# TRAIN LABEL VERIFICATION\n",
        "# ==========================================================\n",
        "\n",
        "def get_label(train_ds, class_names, label_map):\n",
        "\n",
        "    # Mostrar forma y etiquetas del primer batch\n",
        "    for images, labels in train_ds.take(1):\n",
        "        print(\"\\nShape de las im√°genes:\", images.shape)\n",
        "\n",
        "        # Mostrar 9 im√°genes con sus nombres de clase\n",
        "        plt.figure(figsize=(10, 10))\n",
        "        for i in range(9):\n",
        "            ax = plt.subplot(3, 3, i + 1)\n",
        "            plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
        "            label_idx = labels[i].numpy()\n",
        "            plt.title(label_map[label_idx])\n",
        "            plt.axis(\"off\")\n",
        "        plt.suptitle(\"Etiquetas visualizadas con nombres\", fontsize=14)\n",
        "        plt.show()\n",
        "        break\n",
        "\n",
        "\n",
        "# Ejecutar\n",
        "get_label(train_ds, class_names, label_map)\n"
      ],
      "metadata": {
        "id": "oTNj3jadIgCC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# CREA EL MODELO BASE\n",
        "# ==========================================================\n",
        "def create_model(hp):\n",
        "\n",
        "  #Inicializa el modelo\n",
        "  model = Sequential(name = \"mri-model\")\n",
        "\n",
        "  #Capa de entrada (las im√°genes son de tama√±o variable)\n",
        "  model.add(Input(shape=(None,None,3)))\n",
        "\n",
        "  #Primera capa: Capa de resizing\n",
        "  model.add(Resizing(image_size[0], image_size[1]))\n",
        "\n",
        "  #Segunda capa: Capa de normalizaci√≥n\n",
        "  model.add(Rescaling(1./255))\n",
        "\n",
        "  #Bloque convolucional 1\n",
        "  model.add(Conv2D(\n",
        "      filters=hp.Choice('filters_block1', [32, 64]),\n",
        "      kernel_size=(3,3), activation='relu', padding='same'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(MaxPooling2D((2,2)))\n",
        "\n",
        "  #Bloque convolucional 2\n",
        "  model.add(Conv2D(\n",
        "      filters=hp.Choice('filters_block2', [64, 96]),\n",
        "      kernel_size=(3,3), activation='relu', padding='same'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(MaxPooling2D((2,2)))\n",
        "\n",
        "  #Bloque convolucional 3\n",
        "  model.add(Conv2D(\n",
        "      filters=hp.Choice('filters_block3', [96, 128]),\n",
        "      kernel_size=(3,3), activation='relu', padding='same'\n",
        "  ))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(MaxPooling2D((2,2)))\n",
        "\n",
        "  #Bloque denso\n",
        "  model.add(Flatten())\n",
        "  model.add(Dropout(hp.Choice('dropout_1', [ 0.4, 0.5])))\n",
        "  model.add(Dense(\n",
        "      units=hp.Choice('dense_units', [128, 192]),\n",
        "      activation='relu'))\n",
        "  model.add(Dropout(hp.Choice('dropout_2', [ 0.3, 0.4])))\n",
        "  model.add(Dense(4, activation='softmax',dtype='float32'))\n",
        "\n",
        "  # Optimizador y learning rate\n",
        "  lr = hp.Choice('learning_rate', [ 1e-3, 1e-4])\n",
        "  optimizer = Adam(learning_rate=lr)\n",
        "\n",
        "  #Funci√≥n de p√©rdida\n",
        "  loss = SparseCategoricalCrossentropy(label_smoothing=0.1)\n",
        "\n",
        "  model.compile(\n",
        "      optimizer=optimizer,\n",
        "      loss=loss,\n",
        "      metrics=['accuracy']\n",
        "  )\n",
        "\n",
        "  return model\n",
        "\n",
        "# ==========================================================\n",
        "# BUSQUEDA DE HIPERPAR√ÅMETROS MODELO BASE\n",
        "# ==========================================================\n",
        "\n",
        "tuner = RandomSearch(\n",
        "    create_model,\n",
        "    objective='val_accuracy',\n",
        "    max_trials=5,\n",
        "    executions_per_trial=1,\n",
        "    directory='tuning_results',\n",
        "    project_name='cnn_mri_tuning'\n",
        ")\n",
        "\n",
        "# ==========================================================\n",
        "# ENTRENAMIENTO MODELO BASE\n",
        "# ==========================================================\n",
        "\n",
        "tuner.search(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=8,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "#Mejor modelo\n",
        "tuner.results_summary()\n",
        "best_model = tuner.get_best_models(num_models=1)[0]"
      ],
      "metadata": {
        "id": "1RFM0cWSMCpZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Resumen\n",
        "best_model.summary()"
      ],
      "metadata": {
        "id": "LFSHSjuEqmaB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot model\n",
        "plot_model(best_model, show_shapes=True, show_layer_names=False)"
      ],
      "metadata": {
        "id": "N-azu5vPqxH3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Hiperpar√°metros del mejor modelo base\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "print(\"Mejores hiperpar√°metros encontrados:\")\n",
        "for key, value in best_hps.values.items():\n",
        "    print(f\" - {key}: {value}\")"
      ],
      "metadata": {
        "id": "EOf1GA7Krn_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# MEJORA MODELO BASE\n",
        "# ==========================================================\n",
        "\n",
        "# Detiene el entrenamiento si no mejora la val_loss\n",
        "early_stop = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=7,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Reduce el learning rate si la validaci√≥n deja de mejorar\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.5,\n",
        "    patience=3,\n",
        "    min_lr=1e-6,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Guarda el mejor modelo durante el entrenamiento\n",
        "checkpoint = ModelCheckpoint(\n",
        "    'best_model_v2.keras',\n",
        "    monitor='val_accuracy',\n",
        "    save_best_only=True,\n",
        "    mode='max',\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "#Entrenamiento\n",
        "history = best_model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=30,\n",
        "    callbacks=[early_stop, reduce_lr, checkpoint],\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "id": "xatcAl_dsDp0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Visualizaci√≥n de la mejora\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.plot(history.history['accuracy'], label='Entrenamiento')\n",
        "plt.plot(history.history['val_accuracy'], label='Validaci√≥n')\n",
        "plt.title('Accuracy por √âpoca')\n",
        "plt.xlabel('√âpoca')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.plot(history.history['loss'], label='Entrenamiento')\n",
        "plt.plot(history.history['val_loss'], label='Validaci√≥n')\n",
        "plt.title('P√©rdida (Loss) por √âpoca')\n",
        "plt.xlabel('√âpoca')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZXCxjlIGswxh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}