{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO9nMgrIo0bjwsZlewwuqms",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/katterine2558/mri-tumor-detection-cnn/blob/main/mri_tumor_detection_cnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#==========================================================\n",
        "# LIBRARIES\n",
        "#==========================================================\n",
        "from google.colab import files\n",
        "#!pip install -q kaggle #Descomentar si no est√° instalado Kaggle en el entorno\n",
        "import os\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, Flatten, Resizing, Rescaling, RandomBrightness, RandomContrast, RandomRotation, RandomZoom, BatchNormalization, Dropout\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "from tensorflow.keras.metrics import AUC\n",
        "\n",
        "\n",
        "#Ruta base\n",
        "base_dir = '/content/mri_data'\n",
        "#Ruta para almacenar split\n",
        "split_dir = \"/content/mri_data_split\"\n",
        "#Semilla\n",
        "seed = 0;\n",
        "#Tama√±o del batch\n",
        "batch_size = 32\n",
        "#Image size\n",
        "image_size = (224, 224)"
      ],
      "metadata": {
        "id": "GBZzywFcr1rD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#==========================================================\n",
        "# DOWNLOAD DATASET\n",
        "#==========================================================\n",
        "def download_mri_data(base_dir:str):\n",
        "\n",
        "  files.upload() #APIKey de Kaggle\n",
        "\n",
        "  !mkdir -p ~/.kaggle\n",
        "  !mv kaggle.json ~/.kaggle/\n",
        "  !chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "  # Descargar el dataset\n",
        "  !mkdir -p /content/mri_data\n",
        "  !kaggle datasets download -d rm1000/brain-tumor-mri-scans -p /content/mri_data --unzip\n",
        "\n",
        "  # Listar categor√≠as\n",
        "  categories = os.listdir(base_dir)\n",
        "  print(\"Categor√≠as encontradas:\", categories)\n",
        "\n",
        "  # Contar im√°genes por clase\n",
        "  for c in categories:\n",
        "      path = os.path.join(base_dir, c)\n",
        "      print(f\"{c}: {len(os.listdir(path))} im√°genes\")\n",
        "\n",
        "  fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
        "  for i, c in enumerate(categories):\n",
        "      folder = os.path.join(base_dir, c)\n",
        "      img_name = random.choice(os.listdir(folder))\n",
        "      img_path = os.path.join(folder, img_name)\n",
        "      img = cv2.imread(img_path)\n",
        "      img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "      axes[i].imshow(img)\n",
        "      axes[i].set_title(c.capitalize())\n",
        "      axes[i].axis('off')\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "#Descarga el conjunto de datos\n",
        "download_mri_data(base_dir)"
      ],
      "metadata": {
        "id": "JQOc_LcWsUpH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#==========================================================\n",
        "# IMAGE SIZE SCANNER\n",
        "#==========================================================\n",
        "\n",
        "def get_images_size(base_dir:str):\n",
        "\n",
        "  all_sizes = []  # guardamos (ancho, alto)\n",
        "  per_class_count = {}\n",
        "\n",
        "  for cls in os.listdir(base_dir):\n",
        "      cls_path = os.path.join(base_dir, cls)\n",
        "      if not os.path.isdir(cls_path):\n",
        "          continue\n",
        "\n",
        "      imgs = os.listdir(cls_path)\n",
        "      per_class_count[cls] = len(imgs)\n",
        "\n",
        "      # vamos a mirar hasta 20 im√°genes por clase (para no leer miles)\n",
        "      for img_name in imgs[:20]:\n",
        "          img_path = os.path.join(cls_path, img_name)\n",
        "          img = cv2.imread(img_path)  # esto lee en BGR\n",
        "          if img is None:\n",
        "              # a veces hay archivos raros tipo .txt o corruptos\n",
        "              continue\n",
        "          h, w, ch = img.shape  # alto, ancho, canales\n",
        "          all_sizes.append((w, h, ch))\n",
        "\n",
        "\n",
        "  print(\"\\nEjemplos de tama√±os (ancho x alto x canales) en las primeras im√°genes revisadas:\")\n",
        "  for s in all_sizes[:10]:\n",
        "      print(\" \", s)\n",
        "\n",
        "  # estad√≠sticas m√°s globales\n",
        "  widths = [s[0] for s in all_sizes]\n",
        "  heights = [s[1] for s in all_sizes]\n",
        "  channels = [s[2] for s in all_sizes]\n",
        "\n",
        "  print(\"\\nEstad√≠sticas:\")\n",
        "  print(f\"  Ancho min / max / promedio: {min(widths)} / {max(widths)} / {np.mean(widths):.1f}\")\n",
        "  print(f\"  Alto  min / max / promedio: {min(heights)} / {max(heights)} / {np.mean(heights):.1f}\")\n",
        "\n",
        "  print(\"\\nCanales m√°s comunes (1=gris, 3=RGB):\")\n",
        "  print(Counter(channels))\n",
        "\n",
        "#Obtiene el tama√±o de las im√°genes\n",
        "get_images_size(base_dir)"
      ],
      "metadata": {
        "id": "uDvO1Exeyn1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#==========================================================\n",
        "# TRAIN-VAL-TEST DIVISION (70-20-10)\n",
        "#==========================================================\n",
        "\n",
        "def traint_val_test_split(base_dir:str, split_dir:str):\n",
        "\n",
        "  # Crear carpetas destino\n",
        "  for split in ['train', 'val', 'test']:\n",
        "      for cls in os.listdir(base_dir):\n",
        "          os.makedirs(os.path.join(split_dir, split, cls), exist_ok=True)\n",
        "\n",
        "  # Ratios\n",
        "  train_ratio = 0.7\n",
        "  val_ratio = 0.2\n",
        "  test_ratio = 0.1\n",
        "\n",
        "  # Procesar cada clase\n",
        "  for cls in os.listdir(base_dir):\n",
        "      cls_path = os.path.join(base_dir, cls)\n",
        "      images = [f for f in os.listdir(cls_path) if f.lower().endswith(('.png','.jpg','.jpeg'))]\n",
        "\n",
        "      # Dividir train y (val+test)\n",
        "      train_files, temp_files = train_test_split(images, test_size=(1-train_ratio), random_state=seed)\n",
        "\n",
        "      # Dividir en val y test\n",
        "      val_size = val_ratio / (val_ratio + test_ratio)\n",
        "      val_files, test_files = train_test_split(temp_files, test_size=(1-val_size), random_state=seed)\n",
        "\n",
        "      # Copiar archivos\n",
        "      for fname in train_files:\n",
        "          shutil.copy(os.path.join(cls_path, fname), os.path.join(split_dir, 'train', cls, fname))\n",
        "      for fname in val_files:\n",
        "          shutil.copy(os.path.join(cls_path, fname), os.path.join(split_dir, 'val', cls, fname))\n",
        "      for fname in test_files:\n",
        "          shutil.copy(os.path.join(cls_path, fname), os.path.join(split_dir, 'test', cls, fname))\n",
        "\n",
        "  print(\"Divisi√≥n completada con semilla =\", seed)\n",
        "  print(\"Estructura creada en:\", split_dir)\n",
        "\n",
        "  # Verificaci√≥n de conteo\n",
        "  for split in ['train', 'val', 'test']:\n",
        "      print(f\"\\n * {split.upper()}\")\n",
        "      for cls in os.listdir(os.path.join(split_dir, split)):\n",
        "          count = len(os.listdir(os.path.join(split_dir, split, cls)))\n",
        "          print(f\"  {cls}: {count} im√°genes\")\n",
        "\n",
        "#Divisi√≥n traint/val/test\n",
        "traint_val_test_split(base_dir, split_dir)\n",
        "\n",
        "#Carga los datos de validaci√≥n y test\n",
        "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "        f'{split_dir}/val',\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        seed=seed,\n",
        "        image_size=image_size\n",
        "    )\n",
        "test_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "        f'{split_dir}/test',\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        seed=seed,\n",
        "        image_size=image_size\n",
        "    )"
      ],
      "metadata": {
        "id": "cEMADKmdWuvX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# TRAIN DATA AUGMENTATION\n",
        "# ==========================================================\n",
        "\n",
        "def data_augmentation(split_dir:str, seed:int, batch_size:int, image_size:tuple):\n",
        "    # Cargar im√°genes (se necesita tama√±o fijo)\n",
        "    train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "        f'{split_dir}/train',\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        seed=seed,\n",
        "        image_size=image_size\n",
        "    )\n",
        "\n",
        "    class_names = train_ds.class_names\n",
        "    print(\"Clases detectadas:\", class_names)\n",
        "\n",
        "    # üîπ Crear mapeo expl√≠cito de √≠ndice ‚Üí nombre\n",
        "    label_map = {i: name for i, name in enumerate(class_names)}\n",
        "    print(\"\\nüìò Mapeo de etiquetas:\")\n",
        "    for key, value in label_map.items():\n",
        "        print(f\"{key} ‚Üí {value}\")\n",
        "\n",
        "    # Definir Data Augmentation\n",
        "    data_augmentation = tf.keras.Sequential([\n",
        "        RandomRotation(0.05),\n",
        "        RandomZoom(0.1),\n",
        "        RandomContrast(0.1),\n",
        "        RandomBrightness(0.1),\n",
        "    ])\n",
        "\n",
        "    # Aplicar aumentaci√≥n\n",
        "    train_ds = train_ds.map(lambda x, y: (data_augmentation(x, training=True), y))\n",
        "\n",
        "    print(\"\\n‚úÖ Data augmentation aplicada al conjunto de entrenamiento.\")\n",
        "    return train_ds, data_augmentation, class_names, label_map\n",
        "\n",
        "\n",
        "def plot_data_augmentation(train_ds, data_augmentation):\n",
        "    for images, labels in train_ds.take(1):\n",
        "        plt.figure(figsize=(10, 10))\n",
        "        for i in range(9):\n",
        "            augmented = data_augmentation(images, training=True)\n",
        "            ax = plt.subplot(3, 3, i + 1)\n",
        "            plt.imshow(augmented[i].numpy().astype(\"uint8\"))\n",
        "            plt.axis(\"off\")\n",
        "        plt.suptitle(\"Ejemplos de Data Augmentation\", fontsize=14)\n",
        "        plt.show()\n",
        "        break\n",
        "\n",
        "\n",
        "# Ejecutar data augmentation\n",
        "train_ds, data_aug, class_names, label_map = data_augmentation(split_dir, seed, batch_size, image_size)\n",
        "plot_data_augmentation(train_ds, data_aug)"
      ],
      "metadata": {
        "id": "hLR5Mm83cO1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# TRAIN LABEL VERIFICATION\n",
        "# ==========================================================\n",
        "\n",
        "def get_label(train_ds, class_names, label_map):\n",
        "    # Mostrar mapeo en tabla\n",
        "    df = pd.DataFrame({\n",
        "        \"Etiqueta num√©rica\": list(label_map.keys()),\n",
        "        \"Clase\": list(label_map.values())\n",
        "    })\n",
        "    print(\"Mapeo de etiquetas num√©ricas a nombres de clase:\\n\")\n",
        "    display(df)\n",
        "\n",
        "    # Mostrar forma y etiquetas del primer batch\n",
        "    for images, labels in train_ds.take(1):\n",
        "        print(\"\\nShape de las im√°genes:\", images.shape)\n",
        "\n",
        "        # Mostrar 9 im√°genes con sus nombres de clase\n",
        "        plt.figure(figsize=(10, 10))\n",
        "        for i in range(9):\n",
        "            ax = plt.subplot(3, 3, i + 1)\n",
        "            plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
        "            label_idx = labels[i].numpy()\n",
        "            plt.title(label_map[label_idx])\n",
        "            plt.axis(\"off\")\n",
        "        plt.suptitle(\"Etiquetas visualizadas con nombres\", fontsize=14)\n",
        "        plt.show()\n",
        "        break\n",
        "\n",
        "\n",
        "# Ejecutar\n",
        "get_label(train_ds, class_names, label_map)\n"
      ],
      "metadata": {
        "id": "oTNj3jadIgCC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# CREA EL MODELO\n",
        "# ==========================================================\n",
        "def create_model(hp):\n",
        "\n",
        "  #Inicializa el modelo\n",
        "  model = Sequential(name = \"mri-model\")\n",
        "\n",
        "  #Capa de entrada (las im√°genes son de tama√±o variable)\n",
        "  model.add(Input(shape=(None,None,3)))\n",
        "\n",
        "  #Primera capa: Capa de resizing\n",
        "  model.add(Resizing(image_size[0], image_size[1]))\n",
        "\n",
        "  #Segunda capa: Capa de normalizaci√≥n\n",
        "  model.add(Rescaling(1./255))\n",
        "\n",
        "  #Bloque convolucional 1\n",
        "  model.add(Conv2D(\n",
        "      filters=hp.Choice('filters_block1', [32, 64, 96]),\n",
        "      kernel_size=(3,3), activation='relu', padding='same'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(MaxPooling2D((2,2)))\n",
        "\n",
        "  #Bloque convolucional 2\n",
        "  model.add(Conv2D(\n",
        "      filters=hp.Choice('filters_block2', [64, 96, 128]),\n",
        "      kernel_size=(3,3), activation='relu', padding='same'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(MaxPooling2D((2,2)))\n",
        "\n",
        "  #Bloque convolucional 3\n",
        "  model.add(Conv2D(\n",
        "      filters=hp.Choice('filters_block3', [96, 128, 160]),\n",
        "      kernel_size=(3,3), activation='relu', padding='same'\n",
        "  ))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(MaxPooling2D((2,2)))\n",
        "\n",
        "  #Bloque denso\n",
        "  model.add(Flatten())\n",
        "  model.add(Dropout(hp.Choice('dropout_1', [0.3, 0.4, 0.5])))\n",
        "  model.add(Dense(\n",
        "      units=hp.Choice('dense_units', [128, 192, 256]),\n",
        "      activation='relu'))\n",
        "  model.add(Dropout(hp.Choice('dropout_2', [0.2, 0.3, 0.4])))\n",
        "  model.add(Dense(4, activation='softmax'))\n",
        "\n",
        "  # Optimizador y learning rate\n",
        "  lr = hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])\n",
        "  optimizer = Adam(learning_rate=lr)\n",
        "\n",
        "  #Funci√≥n de p√©rdida\n",
        "  loss = SparseCategoricalCrossentropy()\n",
        "\n",
        "  model.compile(\n",
        "      optimizer=optimizer,\n",
        "      loss=loss,\n",
        "      metrics=['accuracy',\n",
        "               AUC(multi_label=True, num_labels=4, name='auc')]\n",
        "  )\n",
        "\n",
        "  return model\n",
        "\n",
        "# ==========================================================\n",
        "# BUSQUEDA DE HIPERPAR√ÅMETROS\n",
        "# ==========================================================\n",
        "\n",
        "tuner = RandomSearch(\n",
        "    create_model,\n",
        "    objective='val_auc',\n",
        "    max_trials=5,\n",
        "    executions_per_trial=1,\n",
        "    directory='tuning_results',\n",
        "    project_name='cnn_mri_tuning'\n",
        ")\n",
        "\n",
        "# ==========================================================\n",
        "# ENTRENAMIENTO\n",
        "# ==========================================================\n",
        "\n",
        "tuner.search(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=8,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "#Mejor modelo\n",
        "tuner.results_summary()\n",
        "best_model = tuner.get_best_models(num_models=1)[0]"
      ],
      "metadata": {
        "id": "1RFM0cWSMCpZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}