{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNOQk+67ZChw3vSXPdzF3oD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/katterine2558/mri-tumor-detection-cnn/blob/main/mri_tumor_detection_cnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# üîß CONFIGURACI√ìN DE ENTORNO COLAB PRO (GPU + OPTIMIZACIONES)\n",
        "# ==========================================================\n",
        "import os, gc, psutil, tensorflow as tf\n",
        "from tensorflow.keras import mixed_precision\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# LIMPIEZA DE SESI√ìN ANTERIOR\n",
        "# ----------------------------------------------------------\n",
        "gc.collect()\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# INFORMACI√ìN DEL HARDWARE\n",
        "# ----------------------------------------------------------\n",
        "try:\n",
        "    gpu_info = !nvidia-smi --query-gpu=name,memory.total --format=csv,noheader\n",
        "    print(\"GPU detectada:\", gpu_info[0])\n",
        "except:\n",
        "    print(\"No se detect√≥ GPU, verifica que el acelerador est√© activado en Entorno > Configuraci√≥n de ejecuci√≥n.\")\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# FORZAR USO DE GPU\n",
        "# ----------------------------------------------------------\n",
        "physical_gpus = tf.config.list_physical_devices('GPU')\n",
        "if physical_gpus:\n",
        "    print(f\"TensorFlow detecta {len(physical_gpus)} GPU(s): {physical_gpus}\")\n",
        "    try:\n",
        "        for gpu in physical_gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "    except:\n",
        "        pass\n",
        "else:\n",
        "    print(\"No hay GPU activa, se usar√° CPU (entrenamiento m√°s lento).\")\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# ACTIVAR ENTRENAMIENTO DE PRECISI√ìN MIXTA\n",
        "# ----------------------------------------------------------\n",
        "mixed_precision.set_global_policy('mixed_float16')\n",
        "print(\"Pol√≠tica de precisi√≥n:\", mixed_precision.global_policy())\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# MOSTRAR INFORMACI√ìN DE RECURSOS\n",
        "# ----------------------------------------------------------\n",
        "print(f\"Memoria RAM disponible: {psutil.virtual_memory().total / 1e9:.2f} GB\")\n",
        "print(\"Entorno configurado correctamente para m√°ximo rendimiento.\\n\")"
      ],
      "metadata": {
        "id": "KmvtUb7v10FA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#==========================================================\n",
        "# LIBRARIES\n",
        "#==========================================================\n",
        "from google.colab import files\n",
        "#!pip install -q kaggle #Descomentar si no est√° instalado Kaggle en el entorno\n",
        "import os\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, Add, Activation,GlobalAveragePooling2D , Multiply, Reshape, DepthwiseConv2D, BatchNormalization ,GlobalAveragePooling2D, Resizing, Rescaling, RandomBrightness, RandomContrast, RandomRotation, RandomZoom, BatchNormalization, Dropout\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.optimizers import Adam, AdamW\n",
        "from tensorflow.keras.initializers import HeNormal\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "from tensorflow.keras.metrics import AUC\n",
        "!pip install keras-tuner --upgrade\n",
        "from keras_tuner.tuners import RandomSearch\n",
        "from tensorflow.keras import mixed_precision\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
        "import seaborn as sns\n",
        "!nvidia-smi\n",
        "\n",
        "#Ruta base\n",
        "base_dir = '/content/mri_data'\n",
        "#Ruta para almacenar split\n",
        "split_dir = \"/content/mri_data_split\"\n",
        "#Semilla\n",
        "seed = 0;\n",
        "#Tama√±o del batch\n",
        "batch_size = 16\n",
        "#Image size\n",
        "image_size = (260, 260)\n",
        "\n",
        "print(\"Dispositivo:\", tf.config.list_physical_devices('GPU'))\n",
        "mixed_precision.set_global_policy('mixed_float16')\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "tf.debugging.set_log_device_placement(False)\n",
        "\n",
        "# Carpeta donde guardas los resultados\n",
        "tuner_dir = \"tuning_results\"\n",
        "\n",
        "# Elimina la carpeta completa si existe\n",
        "if os.path.exists(tuner_dir):\n",
        "    shutil.rmtree(tuner_dir)"
      ],
      "metadata": {
        "id": "GBZzywFcr1rD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#==========================================================\n",
        "# DOWNLOAD DATASET\n",
        "#==========================================================\n",
        "def download_mri_data(base_dir:str):\n",
        "\n",
        "  files.upload() #APIKey de Kaggle\n",
        "\n",
        "  !mkdir -p ~/.kaggle\n",
        "  !mv kaggle.json ~/.kaggle/\n",
        "  !chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "  # Descargar el dataset\n",
        "  !mkdir -p /content/mri_data\n",
        "  !kaggle datasets download -d rm1000/brain-tumor-mri-scans -p /content/mri_data --unzip\n",
        "\n",
        "  # Listar categor√≠as\n",
        "  categories = os.listdir(base_dir)\n",
        "  print(\"Categor√≠as encontradas:\", categories)\n",
        "\n",
        "  # Contar im√°genes por clase\n",
        "  for c in categories:\n",
        "      path = os.path.join(base_dir, c)\n",
        "      print(f\"{c}: {len(os.listdir(path))} im√°genes\")\n",
        "\n",
        "  fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
        "  for i, c in enumerate(categories):\n",
        "      folder = os.path.join(base_dir, c)\n",
        "      img_name = random.choice(os.listdir(folder))\n",
        "      img_path = os.path.join(folder, img_name)\n",
        "      img = cv2.imread(img_path)\n",
        "      img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "      axes[i].imshow(img)\n",
        "      axes[i].set_title(c.capitalize())\n",
        "      axes[i].axis('off')\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "#Descarga el conjunto de datos\n",
        "download_mri_data(base_dir)"
      ],
      "metadata": {
        "id": "JQOc_LcWsUpH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#==========================================================\n",
        "# IMAGE SIZE SCANNER\n",
        "#==========================================================\n",
        "\n",
        "def get_images_size(base_dir:str):\n",
        "\n",
        "  all_sizes = []  # guardamos (ancho, alto)\n",
        "  per_class_count = {}\n",
        "\n",
        "  for cls in os.listdir(base_dir):\n",
        "      cls_path = os.path.join(base_dir, cls)\n",
        "      if not os.path.isdir(cls_path):\n",
        "          continue\n",
        "\n",
        "      imgs = os.listdir(cls_path)\n",
        "      per_class_count[cls] = len(imgs)\n",
        "\n",
        "      # vamos a mirar hasta 20 im√°genes por clase (para no leer miles)\n",
        "      for img_name in imgs[:20]:\n",
        "          img_path = os.path.join(cls_path, img_name)\n",
        "          img = cv2.imread(img_path)  # esto lee en BGR\n",
        "          if img is None:\n",
        "              # a veces hay archivos raros tipo .txt o corruptos\n",
        "              continue\n",
        "          h, w, ch = img.shape  # alto, ancho, canales\n",
        "          all_sizes.append((w, h, ch))\n",
        "\n",
        "\n",
        "  print(\"\\nEjemplos de tama√±os (ancho x alto x canales) en las primeras im√°genes revisadas:\")\n",
        "  for s in all_sizes[:10]:\n",
        "      print(\" \", s)\n",
        "\n",
        "  # estad√≠sticas m√°s globales\n",
        "  widths = [s[0] for s in all_sizes]\n",
        "  heights = [s[1] for s in all_sizes]\n",
        "  channels = [s[2] for s in all_sizes]\n",
        "\n",
        "  print(\"\\nEstad√≠sticas:\")\n",
        "  print(f\"  Ancho min / max / promedio: {min(widths)} / {max(widths)} / {np.mean(widths):.1f}\")\n",
        "  print(f\"  Alto  min / max / promedio: {min(heights)} / {max(heights)} / {np.mean(heights):.1f}\")\n",
        "\n",
        "  print(\"\\nCanales m√°s comunes (1=gris, 3=RGB):\")\n",
        "  print(Counter(channels))\n",
        "\n",
        "#Obtiene el tama√±o de las im√°genes\n",
        "get_images_size(base_dir)"
      ],
      "metadata": {
        "id": "uDvO1Exeyn1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# TRAIN-VAL-TEST DIVISION (70-20-10)\n",
        "# ==========================================================\n",
        "\n",
        "def traint_val_test_split(base_dir:str, split_dir:str):\n",
        "    # Crear carpetas destino\n",
        "    for split in ['train', 'val', 'test']:\n",
        "        for cls in os.listdir(base_dir):\n",
        "            os.makedirs(os.path.join(split_dir, split, cls), exist_ok=True)\n",
        "\n",
        "    # Ratios\n",
        "    train_ratio = 0.7\n",
        "    val_ratio = 0.2\n",
        "    test_ratio = 0.1\n",
        "\n",
        "    # Procesar cada clase\n",
        "    for cls in os.listdir(base_dir):\n",
        "        cls_path = os.path.join(base_dir, cls)\n",
        "        images = [f for f in os.listdir(cls_path) if f.lower().endswith(('.png','.jpg','.jpeg'))]\n",
        "\n",
        "        # Dividir train / val / test\n",
        "        train_files, temp_files = train_test_split(images, test_size=(1-train_ratio), random_state=seed)\n",
        "        val_size = val_ratio / (val_ratio + test_ratio)\n",
        "        val_files, test_files = train_test_split(temp_files, test_size=(1-val_size), random_state=seed)\n",
        "\n",
        "        # Copiar archivos\n",
        "        for fname in train_files:\n",
        "            shutil.copy(os.path.join(cls_path, fname), os.path.join(split_dir, 'train', cls, fname))\n",
        "        for fname in val_files:\n",
        "            shutil.copy(os.path.join(cls_path, fname), os.path.join(split_dir, 'val', cls, fname))\n",
        "        for fname in test_files:\n",
        "            shutil.copy(os.path.join(cls_path, fname), os.path.join(split_dir, 'test', cls, fname))\n",
        "\n",
        "    print(\"Divisi√≥n completada con semilla =\", seed)\n",
        "    print(\"Estructura creada en:\", split_dir)\n",
        "\n",
        "    for split in ['train', 'val', 'test']:\n",
        "        print(f\"\\n * {split.upper()}\")\n",
        "        for cls in os.listdir(os.path.join(split_dir, split)):\n",
        "            count = len(os.listdir(os.path.join(split_dir, split, cls)))\n",
        "            print(f\"  {cls}: {count} im√°genes\")\n",
        "\n",
        "\n",
        "# Ejecutar divisi√≥n solo una vez\n",
        "traint_val_test_split(base_dir, split_dir)\n",
        "\n",
        "# ==========================================================\n",
        "# CARGA DE LOS DATASETS\n",
        "# ==========================================================\n",
        "train_ds_raw = tf.keras.utils.image_dataset_from_directory(\n",
        "    f\"{split_dir}/train\",\n",
        "    image_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    seed=seed\n",
        ")\n",
        "\n",
        "val_ds_raw = tf.keras.utils.image_dataset_from_directory(\n",
        "    f\"{split_dir}/val\",\n",
        "    image_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    seed=seed\n",
        ")\n",
        "\n",
        "test_ds_raw = tf.keras.utils.image_dataset_from_directory(\n",
        "    f\"{split_dir}/test\",\n",
        "    image_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    seed=seed\n",
        ")\n",
        "\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "def prepare_dataset(ds, shuffle=False):\n",
        "    if shuffle:\n",
        "        ds = ds.shuffle(buffer_size=1000)\n",
        "    ds = ds.prefetch(AUTOTUNE)\n",
        "    return ds\n",
        "\n",
        "train_ds = prepare_dataset(train_ds_raw, shuffle=True)\n",
        "val_ds   = prepare_dataset(val_ds_raw)\n",
        "test_ds  = prepare_dataset(test_ds_raw)\n",
        "\n",
        "print(\"Datasets listos y optimizados para entrenamiento.\")"
      ],
      "metadata": {
        "id": "cEMADKmdWuvX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# TRAIN DATA AUGMENTATION\n",
        "# ==========================================================\n",
        "\n",
        "def data_augmentation(split_dir:str, seed:int, batch_size:int, image_size:tuple):\n",
        "    # Cargar im√°genes (se necesita tama√±o fijo)\n",
        "    train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "        f'{split_dir}/train',\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        seed=seed,\n",
        "        image_size=image_size\n",
        "    )\n",
        "\n",
        "    class_names = train_ds.class_names\n",
        "    print(\"Clases detectadas:\", class_names)\n",
        "\n",
        "    # Crear mapeo expl√≠cito de √≠ndice ‚Üí nombre\n",
        "    label_map = {i: name for i, name in enumerate(class_names)}\n",
        "    print(\"\\nMapeo de etiquetas:\")\n",
        "    for key, value in label_map.items():\n",
        "        print(f\"{key} ‚Üí {value}\")\n",
        "\n",
        "    # Definir Data Augmentation\n",
        "    data_augmentation = tf.keras.Sequential([\n",
        "        RandomRotation(0.05),\n",
        "        RandomZoom(0.1),\n",
        "        RandomContrast(0.1),\n",
        "        RandomBrightness(0.1),\n",
        "    ])\n",
        "\n",
        "    # Aplicar aumentaci√≥n\n",
        "    train_ds = train_ds.map(lambda x, y: (data_augmentation(x, training=True), y))\n",
        "\n",
        "    print(\"\\n Data augmentation aplicada al conjunto de entrenamiento.\")\n",
        "    return train_ds, data_augmentation, class_names, label_map\n",
        "\n",
        "\n",
        "def plot_data_augmentation(train_ds, data_augmentation):\n",
        "    for images, labels in train_ds.take(1):\n",
        "        plt.figure(figsize=(10, 10))\n",
        "        for i in range(9):\n",
        "            augmented = data_augmentation(images, training=True)\n",
        "            ax = plt.subplot(3, 3, i + 1)\n",
        "            plt.imshow(augmented[i].numpy().astype(\"uint8\"))\n",
        "            plt.axis(\"off\")\n",
        "        plt.suptitle(\"Ejemplos de Data Augmentation\", fontsize=14)\n",
        "        plt.show()\n",
        "        break\n",
        "\n",
        "\n",
        "# Ejecutar data augmentation\n",
        "train_ds, data_aug, class_names, label_map = data_augmentation(split_dir, seed, batch_size, image_size)\n",
        "plot_data_augmentation(train_ds, data_aug)"
      ],
      "metadata": {
        "id": "hLR5Mm83cO1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# TRAIN LABEL VERIFICATION\n",
        "# ==========================================================\n",
        "\n",
        "def get_label(train_ds, class_names, label_map):\n",
        "\n",
        "    # Mostrar forma y etiquetas del primer batch\n",
        "    for images, labels in train_ds.take(1):\n",
        "        print(\"\\nShape de las im√°genes:\", images.shape)\n",
        "\n",
        "        # Mostrar 9 im√°genes con sus nombres de clase\n",
        "        plt.figure(figsize=(10, 10))\n",
        "        for i in range(9):\n",
        "            ax = plt.subplot(3, 3, i + 1)\n",
        "            plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
        "            label_idx = labels[i].numpy()\n",
        "            plt.title(label_map[label_idx])\n",
        "            plt.axis(\"off\")\n",
        "        plt.suptitle(\"Etiquetas visualizadas con nombres\", fontsize=14)\n",
        "        plt.show()\n",
        "        break\n",
        "\n",
        "\n",
        "# Ejecutar\n",
        "get_label(train_ds, class_names, label_map)\n",
        "\n",
        "for images, labels in val_ds.take(1):\n",
        "    print(\"Batch shape:\", images.shape)\n",
        "    print(\"Labels shape:\", labels.shape)\n"
      ],
      "metadata": {
        "id": "oTNj3jadIgCC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "from tensorflow.keras.optimizers import AdamW\n",
        "from tensorflow.keras.initializers import HeNormal\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "\n",
        "# ==========================================================\n",
        "# ACTIVACI√ìN MISH\n",
        "# ==========================================================\n",
        "@tf.keras.utils.register_keras_serializable()\n",
        "def mish(x):\n",
        "    return x * tf.math.tanh(tf.math.softplus(x))\n",
        "\n",
        "\n",
        "# ==========================================================\n",
        "# BLOQUE MBConv MEJORADO (con DropConnect)\n",
        "# ==========================================================\n",
        "def MBConvMRI(inputs, out_channels, expand_ratio, stride, se_ratio=0.25, drop_rate=0.2):\n",
        "    in_channels = inputs.shape[-1]\n",
        "    shortcut = inputs\n",
        "    x = inputs\n",
        "\n",
        "    expanded = int(in_channels * expand_ratio)\n",
        "    if expand_ratio != 1:\n",
        "        x = layers.Conv2D(expanded, 1, padding='same', use_bias=False, kernel_initializer=HeNormal())(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        x = layers.Activation(mish)(x)\n",
        "\n",
        "    # Depthwise separable\n",
        "    x = layers.DepthwiseConv2D(3, strides=stride, padding='same', use_bias=False, kernel_initializer=HeNormal())(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation(mish)(x)\n",
        "\n",
        "    # Squeeze and Excitation\n",
        "    se = layers.GlobalAveragePooling2D()(x)\n",
        "    se = layers.Reshape((1,1,expanded))(se)\n",
        "    se = layers.Conv2D(int(expanded * se_ratio), 1, activation=mish, padding='same')(se)\n",
        "    se = layers.Conv2D(expanded, 1, activation='sigmoid', padding='same')(se)\n",
        "    x = layers.Multiply()([x, se])\n",
        "\n",
        "    # Proyecci√≥n\n",
        "    x = layers.Conv2D(out_channels, 1, padding='same', use_bias=False, kernel_initializer=HeNormal())(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    # Residual con DropConnect\n",
        "    if stride == 1 and in_channels == out_channels:\n",
        "        if drop_rate > 0:\n",
        "            x = layers.Dropout(drop_rate)(x)\n",
        "        x = layers.Add()([x, shortcut])\n",
        "    return x\n",
        "\n",
        "\n",
        "# ==========================================================\n",
        "# MODELO EfficientMRI-Net v2\n",
        "# ==========================================================\n",
        "def build_efficient_mri_net_v2(num_classes=4, input_shape=(image_size[0], image_size[1], 3)):\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "\n",
        "    # -----------------------------\n",
        "    # Data augmentation integrado\n",
        "    # -----------------------------\n",
        "    x = layers.RandomFlip(\"horizontal\")(inputs)\n",
        "    x = layers.RandomRotation(0.08)(x)\n",
        "    x = layers.RandomZoom(0.1)(x)\n",
        "    x = layers.RandomContrast(0.2)(x)\n",
        "    x = layers.Rescaling(1./255)(x)\n",
        "\n",
        "    # -----------------------------\n",
        "    # Bloque inicial\n",
        "    # -----------------------------\n",
        "    x = layers.Conv2D(48, 3, strides=2, padding='same', use_bias=False, kernel_initializer=HeNormal())(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation(mish)(x)\n",
        "\n",
        "    # -----------------------------\n",
        "    # Bloques MBConv\n",
        "    # -----------------------------\n",
        "    x = MBConvMRI(x, 16, 1, 1, drop_rate=0.1)\n",
        "    x = MBConvMRI(x, 24, 3, 2, drop_rate=0.1)\n",
        "    x = MBConvMRI(x, 24, 3, 1, drop_rate=0.1)\n",
        "    x = MBConvMRI(x, 40, 4, 2, drop_rate=0.15)\n",
        "    x = MBConvMRI(x, 40, 4, 1, drop_rate=0.15)\n",
        "    x = MBConvMRI(x, 80, 4, 2, drop_rate=0.2)\n",
        "    x = MBConvMRI(x, 80, 4, 1, drop_rate=0.2)\n",
        "    x = MBConvMRI(x, 112, 4, 1, drop_rate=0.25)\n",
        "    x = MBConvMRI(x, 160, 4, 2, drop_rate=0.25)\n",
        "    x = MBConvMRI(x, 320, 4, 1, drop_rate=0.3)\n",
        "\n",
        "    # -----------------------------\n",
        "    # Head final\n",
        "    # -----------------------------\n",
        "    x = layers.Conv2D(1280, 1, padding='same', use_bias=False, kernel_initializer=HeNormal())(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation(mish)(x)\n",
        "\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    x = layers.Dropout(0.4)(x)\n",
        "    outputs = layers.Dense(num_classes, activation='softmax', dtype='float32')(x)\n",
        "\n",
        "    model = Model(inputs, outputs, name=\"EfficientMRI_Net_v2\")\n",
        "    return model\n",
        "\n",
        "\n",
        "# ==========================================================\n",
        "# COMPILACI√ìN\n",
        "# ==========================================================\n",
        "model = build_efficient_mri_net_v2(num_classes=4, input_shape=(image_size[0], image_size[1],3))\n",
        "optimizer = AdamW(learning_rate=8e-4, weight_decay=1e-5)\n",
        "loss_fn = SparseCategoricalCrossentropy()\n",
        "\n",
        "model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss=loss_fn,\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# ==========================================================\n",
        "# CALLBACKS\n",
        "# ==========================================================\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.6, patience=5, min_lr=1e-6, verbose=1)\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1)\n",
        "checkpoint = ModelCheckpoint('EfficientMRI_Net_v2.keras', monitor='val_accuracy', save_best_only=True, verbose=1)\n",
        "\n",
        "model.summary()\n",
        "\n",
        "\n",
        "# ==========================================================\n",
        "# ENTRNEAMIENTO\n",
        "# ==========================================================\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=80,\n",
        "    batch_size=batch_size,\n",
        "    callbacks=[reduce_lr, early_stop, checkpoint],\n",
        "    verbose=1\n",
        ")\n"
      ],
      "metadata": {
        "id": "1RFM0cWSMCpZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# GR√ÅFICAS DE ENTRENAMIENTO Y VALIDACI√ìN\n",
        "# ==========================================================\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs_range = range(1, len(acc) + 1)\n",
        "\n",
        "plt.figure(figsize=(14,5))\n",
        "\n",
        "# Accuracy\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, 'b-', label='Entrenamiento')\n",
        "plt.plot(epochs_range, val_acc, 'r--', label='Validaci√≥n')\n",
        "plt.title('Evoluci√≥n de la Accuracy')\n",
        "plt.xlabel('√âpoca')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle='--', alpha=0.5)\n",
        "\n",
        "# Loss\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, 'b-', label='Entrenamiento')\n",
        "plt.plot(epochs_range, val_loss, 'r--', label='Validaci√≥n')\n",
        "plt.title('Evoluci√≥n de la P√©rdida (Loss)')\n",
        "plt.xlabel('√âpoca')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle='--', alpha=0.5)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZXCxjlIGswxh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# EVALUACI√ìN FINAL EN TEST\n",
        "# ==========================================================\n",
        "test_loss, test_acc = model.evaluate(test_ds, verbose=1)\n",
        "print(f\"\\n‚úÖ Test Accuracy: {test_acc:.4f}\")\n",
        "print(f\"üìâ Test Loss: {test_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "qyWjcRYaRmUm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# PREDICCIONES Y REPORTES\n",
        "# ==========================================================\n",
        "# Obtener etiquetas verdaderas y predichas\n",
        "y_true = np.concatenate([y for x, y in test_ds], axis=0)\n",
        "y_pred_probs = model.predict(test_ds)\n",
        "y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "\n",
        "# Etiquetas de las clases\n",
        "class_names = test_ds.class_names\n",
        "print(\"\\nüßæ Reporte de Clasificaci√≥n:\\n\")\n",
        "print(classification_report(y_true, y_pred, target_names=class_names, digits=2))\n",
        "\n",
        "# ==========================================================\n",
        "# MATRIZ DE CONFUSI√ìN\n",
        "# ==========================================================\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "plt.figure(figsize=(7,6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "plt.title(\"Matriz de Confusi√≥n - EfficientMRI-Net\")\n",
        "plt.xlabel(\"Predicci√≥n\")\n",
        "plt.ylabel(\"Real\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "DCjLoldKSyVW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}